{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch基础: 模型参数设置和输入、输出形状"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在PyTorch的torch.nn中，模型类的参数设置的都是输入、输出的`特征`，而输入输出的样本大小（批量大小），则直接由数据决定，不需要在模型中进行参数设置。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一般来说我们用DataLoader, TensorDataset这两个包，\n",
    "# 其中先使用TensorDataset转换数据个数，再放入DataLoader中作为一个生成器\n",
    "x = torch.randn(32, 4)  # x data (torch tensor)\n",
    "y = torch.randn(32, 1)      # y data (torch tensor)\n",
    " \n",
    "# 先转换成 torch 能识别的 Dataset\n",
    "torch_dataset = TensorDataset(x, y)\n",
    " \n",
    "# 把 dataset 放入 DataLoader\n",
    "data_iter = DataLoader(\n",
    "    dataset=torch_dataset,      # torch TensorDataset format\n",
    "    batch_size=3,               # batch size\n",
    "    shuffle=True,               # 要不要打乱数据 (打乱比较好)\n",
    "    num_workers=2)              # 多线程来读数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature: tensor([[-1.0117, -0.8266, -0.6284, -2.5816],\n",
      "        [-0.0178, -0.9856, -1.4027, -0.3659],\n",
      "        [ 0.5260,  0.1485,  0.3743,  0.1258]]) \n",
      " target: tensor([0.2598, 1.8055, 1.7201])\n",
      "data_iter中总共有11个batch的数据。\n"
     ]
    }
   ],
   "source": [
    "# check how data_iter works\n",
    "count = 0 \n",
    "for X, y in data_iter:\n",
    "    if count == 0:\n",
    "        print(\"feature:\", X, \"\\n\", \"target:\", y)\n",
    "    count += 1\n",
    "    continue\n",
    "print(f'data_iter中总共有{count}个batch的数据。')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 首先使用类的格式写一个单层线性神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearModel, self).__init__()\n",
    "        linear = nn.Linear(in_features=4, out_features=1)\n",
    "        linear.weight.data.normal_(0, 0.01)\n",
    "        linear.bias.data.fill_(0)\n",
    "        self.layer = linear\n",
    "    def forward(self, x):\n",
    "        x = self.layer(x)\n",
    "        # x = x.sum(axis=0)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用上例一样的数据\n",
    "x = torch.randn(32, 4)  # x data (torch tensor)\n",
    "y = torch.randn(32, 1)      # y data (torch tensor)\n",
    "# 先转换成 torch 能识别的 Dataset\n",
    "torch_dataset = TensorDataset(x, y)\n",
    "data_iter = DataLoader(\n",
    "    dataset=torch_dataset,      # torch TensorDataset format\n",
    "    batch_size=3,               # batch size\n",
    "    shuffle=True,               # 要不要打乱数据 (打乱比较好)\n",
    "    num_workers=2)              # 多线程来读数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输出形状: torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "lm = LinearModel()\n",
    "# 此时代表32 * 4的自变量x和4 * 1的系数矩阵相乘，保证了相容\n",
    "print(f'输出形状: {lm(x).shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 我们还可以使用nn.Sequential简洁实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输出形状: torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "# construct a linear neural net\n",
    "net = nn.Sequential(nn.Linear(4, 1))\n",
    "\n",
    "# initialization of parameters\n",
    "net[0].weight.data.normal_(0, 0.01)\n",
    "net[0].bias.data.fill_(0)\n",
    "print(f'输出形状: {net(x).shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0113,  0.0072, -0.0092, -0.0127]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# let us check the detailed values of parameters\n",
    "for paras in net.parameters():\n",
    "    print(paras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 损失函数和优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specification of loss function\n",
    "# specification of optimizer\n",
    "loss = nn.MSELoss()\n",
    "optim = torch.optim.SGD(lm.parameters(), lr=0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.3066, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = lm(x)\n",
    "loss(y_hat, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3066014051437378\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# 查看loss的算法：MSE就是误差的平方的平均值\n",
    "print((sum((y - y_hat) ** 2) / len(y)).item())\n",
    "print(round((sum((y - y_hat) ** 2) / len(y)).item(), 5) == round(loss(y_hat, y).item(), 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 反向传播训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0095,  0.0007,  0.0019, -0.0010]], requires_grad=True) tensor([[-19.5573, -32.4925,   1.5564, -22.6102]])\n",
      "Parameter containing:\n",
      "tensor([0.], requires_grad=True) tensor([1.1328])\n"
     ]
    }
   ],
   "source": [
    "# 首先明确的是，在反向传播之前，各个参数的梯度为空\n",
    "for p in lm.parameters():\n",
    "    print(p, p.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 1.289197\n",
      "epoch 2, loss 1.278411\n",
      "epoch 3, loss 1.361941\n"
     ]
    }
   ],
   "source": [
    "# start training \n",
    "# loss backward + opt step\n",
    "num_epochs = 3  # 总共遍历三次全样本\n",
    "for epoch in range(num_epochs):\n",
    "    for X_train, y_train in data_iter:\n",
    "        l = loss(lm(X_train), y_train)\n",
    "        # 对optimizer进行zero_grad操作可以将定义的线性模型的参数的梯度化为0\n",
    "        # 每步进行梯度下降的时候，我们都需要将优化器中保存的参数重置为0，否则这次计算的梯度就会累加到上次的梯度中\n",
    "        optim.zero_grad()\n",
    "        l.backward()\n",
    "        # 优化器走一步，就是根据目前的梯度进行一次梯度下降\n",
    "        optim.step()\n",
    "    l = loss(lm(x), y)\n",
    "    print(f'epoch {epoch + 1}, loss {l:f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成模拟数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating the synthetic data following a logit model\n",
    "\"\"\"⽣成logit(p = 1) = XW + b噪声\"\"\"\n",
    "# 真实参数\n",
    "w = torch.tensor([2, -3.4])\n",
    "b = 1.2\n",
    "num_examples = 30\n",
    "\n",
    "# 根据真实参数，模拟数据\n",
    "X = torch.normal(0, 1, (num_examples, len(w)))\n",
    "prob = torch.matmul(X, w) + b\n",
    "prob = torch.exp(prob) / (1 + torch.exp(prob))\n",
    "\n",
    "# prob是长度30的向量，这里根据每个分量对应生成y∈{0, 1}\n",
    "y = np.random.binomial(np.ones(num_examples, dtype = \"int\"), prob)\n",
    "y = torch.tensor(y).reshape((-1, 1))\n",
    "\n",
    "#  构造数据生成器\n",
    "torch_dataset = TensorDataset(X, y)\n",
    "data_iter = DataLoader(\n",
    "    dataset=torch_dataset,      # torch TensorDataset format\n",
    "    batch_size=3,               # batch size\n",
    "    shuffle=True,               # 要不要打乱数据 (打乱比较好)\n",
    "    num_workers=2)              # 多线程来读数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([30, 2]), torch.Size([30, 1]), torch.Size([30]))"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape, prob.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic 损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic loss function, i.e., the negative log-likelihood\n",
    "def logit_loss(y_hat, y): \n",
    "    \"\"\"logit loss function\"\"\"\n",
    "    # 这个公式的详细推导见Logistic Regression 笔记\n",
    "    return (- y.reshape(y_hat.shape) * y_hat + torch.log(1 + torch.exp(y_hat))).sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 17.202641\n",
      "epoch 2, loss 14.917555\n",
      "epoch 3, loss 13.360222\n",
      "epoch 4, loss 12.256972\n",
      "epoch 5, loss 11.437306\n",
      "epoch 6, loss 10.835018\n",
      "epoch 7, loss 10.369900\n",
      "epoch 8, loss 9.999341\n",
      "epoch 9, loss 9.704663\n",
      "epoch 10, loss 9.456801\n"
     ]
    }
   ],
   "source": [
    "# training procedure\n",
    "loss = logit_loss\n",
    "lm = nn.Sequential(nn.Linear(2, 1))\n",
    "optim = torch.optim.SGD(lm.parameters(), lr=0.03)\n",
    "\n",
    "# tuning parameters\n",
    "lr = 0.1\n",
    "num_epochs = 10\n",
    "\n",
    "# initialization\n",
    "# initialization of parameters\n",
    "lm[0].weight.data.normal_(0, 0.01)\n",
    "lm[0].bias.data.fill_(1)\n",
    "\n",
    "# start training \n",
    "for epoch in range(num_epochs):\n",
    "    for X_train, y_train in data_iter:\n",
    "        y_train = y_train.to(torch.float)\n",
    "        l = loss(lm(X_train), y_train)\n",
    "        # 对optimizer进行zero_grad操作可以将定义的线性模型的参数的梯度化为0\n",
    "        # 每步进行梯度下降的时候，我们都需要将优化器中保存的参数重置为0，否则这次计算的梯度就会累加到上次的梯度中\n",
    "        optim.zero_grad()\n",
    "        l.backward()\n",
    "        # 优化器走一步，就是根据目前的梯度进行一次梯度下降\n",
    "        optim.step()\n",
    "    l = loss(lm(X), y)\n",
    "    print(f'epoch {epoch + 1}, loss {l:f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0], [0, 0, 0], [0, 0, 0]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat = [[0] * 3 for _ in range(3)]\n",
    "mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat[0][1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 0], [0, 0, 0], [0, 0, 0]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
